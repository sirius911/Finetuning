# Fine-tuning Whisper pour la Reconnaissance Automatique de la Parole M√©dicale en Fran√ßais

## *Cyrille LORIN (CH de P√©rigueux)*
Octobre 2024

## R√©sum√©

Cet article explore le processus de fine-tuning du mod√®le **Whisper** pour la transcription de termes m√©dicaux en fran√ßais. Nous avons utilis√© le mod√®le Whisper-base et g√©n√©r√© nos propres donn√©es √† l'aide des mod√®les **LLM Mistral** pour produire des textes m√©dicaux et le **TTS Bark** pour les convertir en audio. Le but de cette √©tude est de montrer que ce proc√©d√© sp√©cifique permet d'am√©liorer les performances d'un mod√®le existant. L'article d√©taille la pr√©paration des donn√©es, l'architecture du mod√®le, et l'√©valuation des performances avec la m√©trique **Word Error Rate** (WER).


## Table des Mati√®res
1. Introduction
2. Fonctionnement de Whisper
3. Versions des Mod√®les Whisper
4. Pr√©paration de l'Environnement Python
5. Cr√©ation des Donn√©es et Validit√©
6. Proc√©dure de Fine-Tuning
7. Adaptation pour whisperLive
8. R√©sultats
9. Conclusion
10. R√©f√©rences

---

## 1. Introduction

La reconnaissance automatique de la parole (ASR) est un domaine en pleine expansion, avec des applications vari√©es notamment dans le secteur m√©dical. Le mod√®le Whisper d‚ÄôOpenAI se distingue par sa capacit√© √† transcrire et √† traduire automatiquement des enregistrements dans plusieurs langues, en se basant sur 680 000 heures de donn√©es audio.

Le besoin de solutions adapt√©es aux donn√©es m√©dicales en fran√ßais a motiv√© l'adaptation de Whisper pour des sc√©narios sp√©cifiques. Cette √©tude explore comment fine-tuner Whisper pour traiter des termes m√©dicaux.

## 2. Fonctionnement de Whisper

Whisper est un mod√®le pr√©-entra√Æn√© pour la reconnaissance automatique de la parole (**ASR**) publi√© en septembre 2022 par OpenAI, con√ßu pour la **transcription** et la **traduction** dans plus de **96 langues**, dont plusieurs √† faibles ressources. Il se distingue par un pr√©-entra√Ænement massif sur **680 000 heures de donn√©es audio √©tiquet√©es**, bien plus que ses pr√©d√©cesseurs comme **Wav2Vec 2.0**, qui utilise 60 000 heures de donn√©es non √©tiquet√©es.

Contrairement aux approches non supervis√©es comme **Wav2Vec 2.0**, Whisper apprend une correspondance directe entre la parole et le texte en √©tant pr√©-entra√Æn√© directement sur des donn√©es √©tiquet√©es. Sur 680 000 heures de pr√©-entra√Ænement, **117 000 heures** concernent l‚Äô**ASR multilingue**, ce qui permet une large g√©n√©ralisation des points de contr√¥le pr√©-entra√Æn√©s √† d‚Äôautres langues et domaines. Ce volume de donn√©es √©tiquet√©es donne √† Whisper un avantage, rendant les ajustements suppl√©mentaires minimaux pour des t√¢ches sp√©cifiques comme le domaine m√©dical.

### Architecture de Whisper

Whisper est bas√© sur un mod√®le **seq2seq** (s√©quence √† s√©quence) avec une architecture **transformer**. Le mod√®le utilise un **codeur-d√©codeur** dans lequel :
- Le **codeur** transforme l‚Äôentr√©e audio (sous forme de spectrogramme log-Mel) en une s√©quence d‚Äô√©tats cach√©s.
- Le **d√©codeur** pr√©dit ensuite les mots de mani√®re autor√©gressive, en se basant sur ces √©tats cach√©s et sur les mots pr√©c√©demment pr√©dits.

#### Fonctionnement
1. **Entr√©e audio** : Le signal audio est d'abord converti en spectrogramme log-Mel via un extracteur de caract√©ristiques.
2. **Encodage** : Le spectrogramme est encod√© par le **transformer** pour g√©n√©rer une s√©quence d'√©tats cach√©s.
3. **D√©codage** : Le d√©codeur pr√©dit la s√©quence de mots de sortie (texte) en fonction des √©tats cach√©s et des jetons de texte pr√©dits pr√©c√©demment.

La **figure 1** ci-dessous illustre cette architecture.

![mod√®le Whisper](images/whisper_architecture.jpg "figure 1")

**Figure 1** : Mod√®le Whisper. L'architecture suit le mod√®le s√©quence-s√©quence transformateur typique. Le spectrogramme est pass√© dans un codeur, dont les √©tats cach√©s sont utilis√©s par le d√©codeur pour produire de mani√®re autor√©gressive les mots du texte. Source : OpenAI Whisper Blog.

### Apprentissage et Ajustement

Whisper utilise une fonction objective standard de l‚Äô**entropie crois√©e**, permettant un apprentissage de bout en bout avec une correspondance parole-texte imm√©diate. Compar√© √† des approches comme **CTC + nn-gram**, qui utilisent une **fusion superficielle** (mod√®le linguistique externe), Whisper int√®gre une **fusion profonde**, o√π le mod√®le linguistique est int√©gr√© au syst√®me, am√©liorant ainsi la flexibilit√© et la performance globale.

Lors de son pr√©-entra√Ænement, Whisper atteint des performances remarquables, notamment un **WER de 3%** sur le sous-ensemble test-propre de **LibriSpeech** et un √©tat de l'art de **4,7%** sur **TED-LIUM**.

Gr√¢ce √† son architecture flexible et ses capacit√©s multilingues, Whisper peut √™tre finement ajust√© pour des langues sp√©cifiques ou des applications sp√©cialis√©es, comme la transcription m√©dicale, avec un ajustement minimal n√©cessaire.

## 3. Versions des Mod√®les Whisper

Whisper est disponible en plusieurs tailles de mod√®le, qui varient en termes de pr√©cision et de consommation de ressources. Nous avons utilis√© **Whisper-base** pour notre projet, car il offre un bon compromis entre pr√©cision et rapidit√© d'entra√Ænement pour cette exp√©rience qui a √©t√© effectu√©e sur un ordinateur de bureau sans carte GPU. Le but √† terme est d'entra√Æner des mod√®les plus larges avec des serveurs de bonne capacit√© autorisant la transcription et les calculs sur beaucoup plus de param√®tres et donc une meilleur performance.

| Version         | Taille (Go) | Param√®tres | Largeur des Couches | Couches d'Attention |
|-----------------|-------------|------------|---------------------|---------------------|
| Whisper-tiny    | 0.15        | 39 M       | 384                 | 4                   |
| Whisper-base    | 0.31        | 74 M       | 512                 | 6                   |
| Whisper-small   | 0.46        | 244 M      | 768                 | 12                  |
| Whisper-medium  | 1.5         | 769 M      | 1024                | 24                  |
| Whisper-large   | 2.9         | 1550 M     | 1280                | 32                  |

## 4. Pr√©paration de l'Environnement Python

Voici la liste des modules Python √† installer pour pr√©parer l'environnement :

1. **`datasets[audio]`** : pour t√©l√©charger et pr√©parer les donn√©es d'entra√Ænement audio.
2. **`transformers`** : pour charger et entra√Æner le mod√®le Whisper.
3. **`accelerate`** : pour acc√©l√©rer l'entra√Ænement du mod√®le.
4. **`evaluate`** : pour √©valuer les performances avec des m√©triques.
5. **`jiwer`** : pour calculer le taux d'erreur de mots (WER).
6. **`tensorboard`** : pour suivre les m√©triques d'entra√Ænement.
7. **`soundfile`** : pour pr√©traiter les fichiers audio.
8. **`pandas`** : pour manipuler des DataFrames lors de la cr√©ation des fichiers CSV d'entra√Ænement et de test.
9. **`sklearn`** : pour diviser les donn√©es en ensembles d'entra√Ænement et de test.
10. **`torch`** : pour g√©rer les tenseurs et entra√Æner le mod√®le Whisper.
11. **`ctranslate2`** : pour convertir le mod√®le en format compatible avec `faster-whisper`.

Cela devrait couvrir tous les modules n√©cessaires pour ex√©cuter les exemples et le code fournis dans le fichier README.
```sh
pip install --upgrade pip
pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard soundfile pandas scikit-learn torch ctranslate2

```
L'utilisation d'un environnement python est conseill√©.
Nous vous conseillons vivement de t√©l√©charger les checkpoints du mod√®le directement sur le Hugging Face Hub pendant l'entra√Ænement. Au moins pour le premier entrainement. Par la suite on peut mettre l'adresse physique du mod√®le d√©j√† t√©l√©charg√© ou le mod√®le personnel auparavant entra√Æn√©.

## 5. Cr√©ation des Donn√©es et Validit√©

### G√©n√©ration de Textes M√©dicaux avec Mistral

En raison du manque de donn√©es m√©dicales vocales authentiques en fran√ßais, nous avons g√©n√©r√© nos propres donn√©es √† l'aide du LLM **Mistral** pour cr√©er des rapports m√©dicaux couvrant des sujets comme les diagnostics et les traitements. Bien que Mistral puisse produire des textes de qualit√© variable et discutable du point de vue m√©dicale, notre objectif est d'exposer Whisper aux termes m√©dicaux dans des contextes vari√©s. Ces textes doivent relativement cours pour ne pas d√©passer √† la lecture 30s. (*voir plus bas*)

### Conversion en Audio avec Bark (Text-to-Speech)

Les textes g√©n√©r√©s par **Mistral** ont ensuite √©t√© convertis en audio gr√¢ce au mod√®le **Text-to-Speech Bark**. Bark simule la lecture des rapports m√©dicaux en utilisant diff√©rentes voix (hommes ou femmes), avec des accents vari√©s, des h√©sitations ou des erreurs, ajoutant ainsi une diversit√© utile aux donn√©es. Cette approche pr√©sente l‚Äôavantage de g√©n√©rer des donn√©es d‚Äôentra√Ænement r√©alistes sans avoir recours √† des voix humaines. Ainsi, en se concentrant uniquement sur la qualit√© des rapports √©crits, nous pourrions anonymiser ces documents et entra√Æner nos mod√®les sur d'importants volumes de donn√©es m√©dicales.

### Structure des Donn√©es

Les donn√©es sont organis√©es de la mani√®re suivante pour √™tre utilis√©es dans le processus de fine-tuning :

```
data/
|  ‚îú‚îÄ‚îÄ audio/
|  ‚îú‚îÄ‚îÄ rapports/
‚îú‚îÄ‚îÄ train.csv
‚îî‚îÄ‚îÄ test.csv
```

Le fichier CSV contient les chemins vers les fichiers audio et les transcriptions correspondantes, s√©par√©s en ensembles d'entra√Ænement et de test. Voici un exemple de code pour g√©n√©rer les fichiers CSV :

```python
import os
import pandas as pd
from sklearn.model_selection import train_test_split

# Define the paths
audio_dir = 'data/audio'
text_dir = 'data/rapports'

# Create a list to hold the data
data = []

# Loop over each file in the audio directory
for audio_file in sorted(os.listdir(audio_dir)):
    if audio_file.endswith('.wav'):  # Assuming the audio files are in .wav format
        # Construct the corresponding text file path
        text_file = os.path.splitext(audio_file)[0] + '.txt'
        text_file_path = os.path.join(text_dir, text_file)
        
        # Read the transcription text if the file exists
        if os.path.exists(text_file_path):
            with open(text_file_path, 'r', encoding='utf-8') as f:
                transcription = f.read().strip()
        
            # Add the information to the data list
            data.append({
                'audio': os.path.join(audio_dir, audio_file),
                'sentence': transcription
            })

# Convert the list to a DataFrame
df = pd.DataFrame(data)

# Split the data into 80% train and 20% test
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Save the DataFrame to separate CSV files for train and test
train_df.to_csv('train.csv', index=False)
test_df.to_csv('test.csv', index=False)

print("Train and Test CSV files created successfully.")
```
```
Train and Test CSV files created successfully.
```

Nous avons r√©parti l'ensemble de nos datas (226) en 80% (180) pour l'entrainement et 20%(46) pour le test.

Nous nous retrouvons donc avec des fichiers csv de la forme : 

nom_du_fichier_audio.wav, '*transcription*'


``` csv
audio,sentence
data/audio/151.wav,"Le rapport biologique r√©v√®le une thyro√Ødite de Hashimoto caract√©ris√©e par une infiltration lymphocytaire et une fibrose diffuse, confirmant une pathologie auto-immune."
```

## 6. Proc√©dure de Fine-Tuning

Une fois l'environnement et les donn√©es pr√™ts, le fine-tuning peut √™tre r√©alis√© en utilisant la biblioth√®que Hugging Face. 

### Charger l'ensemble de donn√©es (**Dataset**)


```python
from datasets import load_dataset, DatasetDict

# Load the local CSV files for train and test sets
data_files = {
    "train": "data/train.csv",  # Local path to train.csv
    "test": "data/test.csv"     # Local path to test.csv
}

# Load the dataset from the local CSV files
dataset = DatasetDict()
dataset["train"] = load_dataset("csv", data_files={"train": data_files["train"]}, split="train")
dataset["test"] = load_dataset("csv", data_files={"test": data_files["test"]}, split="test")

print(dataset)
```
```
DatasetDict({
    train: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 180
    })
    test: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 46
    })
})
```
Nous utilisons le mod√®le **Whisper-base** d'OpenAI ("openai/whisper-base") comme point de d√©part. Lors de la premi√®re utilisation, il est t√©l√©charg√© depuis le hub. Par la suite, nous pouvons sp√©cifier un chemin personnalis√© pour **model_name**, soit pour entra√Æner dans un environnement ferm√©, soit pour r√©entra√Æner un mod√®le d√©j√† fine-tun√© (par exemple, "/home/finetune/finetuned_model").


```python
import torch
from transformers import WhisperForConditionalGeneration, WhisperTokenizer

model_name = "openai/whisper-base"
model = WhisperForConditionalGeneration.from_pretrained(model_name)
tokenizer = WhisperTokenizer.from_pretrained(model_name)
```

## Pr√©parer l'extracteur de caract√©ristiques (Extractor), le tokenizer et les datas

Le pipeline ASR peut √™tre d√©compos√© en trois √©l√©ments :


1.   Un extracteur de caract√©ristiques qui pr√©-traite les entr√©es audio brutes
2.   Le mod√®le qui effectue le mappage s√©quence-s√©quence
3.   Un tokenizer qui post-traite les sorties du mod√®le au format texte.

Dans ü§ó Transformers, le mod√®le Whisper est associ√© √† un extracteur de caract√©ristiques et √† un tokenizer, appel√©s respectivement WhisperFeatureExtractor et WhisperTokenizer.

**Charger l'extracteur de caract√©ristiques WhisperFeatureExtractor**

Whisper utilise un **extracteur de caract√©ristiques** qui effectue deux t√¢ches principales pour pr√©parer les entr√©es audio : 

1. **Uniformisation de la longueur audio** : Chaque fichier audio est ajust√© √† une dur√©e de 30 secondes, soit en remplissant les fichiers plus courts avec des z√©ros (silence), soit en tronquant les fichiers plus longs. Cette approche √©limine le besoin d'un masque d'attention pour identifier les sections remplies, car Whisper d√©duit lui-m√™me les zones √† ignorer.

2. **Transformation en spectrogramme log-Mel** : L‚Äôaudio est ensuite converti en **spectrogramme log-Mel**, une repr√©sentation visuelle des fr√©quences audio dans le temps. Le long de l'axe des ordonn√©es se trouvent les **canaux Mel**, qui repr√©sentent des plages de fr√©quences sp√©cifiques, tandis que l'axe des abscisses repr√©sente le temps. Chaque pixel du spectrogramme refl√®te l‚Äôintensit√© logarithmique de chaque bin de fr√©quence √† un moment donn√©. Cette repr√©sentation est standard dans le traitement de la parole, car elle se rapproche de la perception auditive humaine.

Cette transformation est essentielle pour que Whisper puisse interpr√©ter correctement les entr√©es audio. Le **spectrogramme log-Mel** est la forme d‚Äôentr√©e attendue par le mod√®le Whisper, permettant une compr√©hension plus fine des variations de fr√©quence, comme illustr√© dans la **Figure 2** ci-dessous.

![Spectrogramme](images/spectrogram.jpg)

**Figure 2** : Repr√©sentation d'un spectrogramme log-Mel. √Ä gauche, un signal audio √©chantillonn√© ; √† droite, le spectrogramme correspondant. Les canaux Mel repr√©sentent les fr√©quences per√ßues par l'oreille humaine. Source : [Google SpecAugment Blog.](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)

Le spectrogramme visuel permet d‚Äôanalyser les composantes fr√©quentielles du signal audio √† chaque instant, crucial pour la transcription vocale par Whisper. Gr√¢ce √† l'extracteur de caract√©ristiques de **ü§ó Transformers**, ces op√©rations de padding et de transformation en spectrogramme sont r√©alis√©es en une seule ligne de code, facilitant ainsi la pr√©paration des donn√©es audio pour l'entra√Ænement ou l'inf√©rence du mod√®le.

Chargeons l'extracteur de caract√©ristiques √† partir du point de contr√¥le pr√©-entra√Æn√© pour qu'il soit pr√™t pour nos donn√©es audio :


```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
```

## Load WhisperTokenizer
Voyons maintenant comment charger un tokenizer Whisper. Le mod√®le Whisper produit des jetons de texte qui indiquent l'index du texte pr√©dit dans le dictionnaire des √©l√©ments de vocabulaire. Le tokenizer fait correspondre une s√©quence de jetons de texte √† la cha√Æne de texte r√©elle (par exemple, [1169, 3797, 3332] -> ¬´ le chat s'est assis ¬ª).

Traditionnellement, lors de l'utilisation de mod√®les √† encodeur seul pour l'ASR, nous d√©codons en utilisant la classification temporelle connexionniste ([CTC](https://distill.pub/2017/ctc/)). Dans ce cas, nous devons former un tokenizer CTC pour chaque ensemble de donn√©es que nous utilisons. L'un des avantages de l'utilisation d'une architecture codeur-d√©codeur est que nous pouvons directement exploiter le tokenizer du mod√®le pr√©-entra√Æn√©.

Le tokenizer Whisper est pr√©-entra√Æn√© sur les transcriptions des 96 langues de pr√©-entra√Ænement. Par cons√©quent, il dispose d'une [paire d'octets](https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization) √©tendue qui convient √† presque toutes les applications ASR multilingues. Pour le fran√ßais, nous pouvons charger le tokenizer et l'utiliser pour un r√©glage fin sans aucune autre modification. Il suffit de sp√©cifier la langue cible et la t√¢che. Ces arguments indiquent au tokenizer de pr√©fixer les tokens de la langue et de la t√¢che au d√©but des s√©quences d'√©tiquettes encod√©es :



```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained(model_name, language="French", task="transcribe")
```

Nous pouvons v√©rifier que le tokenizer encode correctement les caract√®res en fran√ßais en encodant et en d√©codant un √©chantillon de l'ensemble de donn√©es. Lors de l'encodage des transcriptions, le tokenizer ajoute des ¬´ jetons sp√©ciaux ¬ª au d√©but et √† la fin de la s√©quence, tels que les jetons de d√©but/fin de transcription, de langue et de t√¢che. Lors du d√©codage, il est possible de ¬´ sauter ¬ª ces jetons pour retourner une cha√Æne similaire √† celle de l'entr√©e originale.


```python
input_str = dataset["train"][0]["sentence"]
labels = tokenizer(input_str).input_ids
decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)
decoded_str = tokenizer.decode(labels, skip_special_tokens=True)

print(f"Input:                 {input_str}")
print(f"Decoded w/ special:    {decoded_with_special}")
print(f"Decoded w/out special: {decoded_str}")
print(f"Are equal:             {input_str == decoded_str}")
```

    Input:                 L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.
    Decoded w/ special:    <|startoftranscript|><|fr|><|transcribe|><|notimestamps|>L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.<|endoftext|>
    Decoded w/out special: L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.
    Are equal:             True

## Combiner pour cr√©er un WhisperProcessor

Pour simplifier l'utilisation de l'extracteur de caract√©ristiques et du tokenizer, nous pouvons les regrouper (*wrap*) dans une seule classe **WhisperProcessor**. Cet objet processeur **h√©rite** des classes **WhisperFeatureExtractor** et WhisperProcessor et peut √™tre utilis√© sur les entr√©es audio et les pr√©dictions du mod√®le selon les besoins. Ce faisant, nous n'avons besoin de suivre que deux objets pendant l'apprentissage : le processeur et le mod√®le :


```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained(model_name, language="French", task="transcribe")
```

**Pr√©parer les donn√©es**

Imprimons le premier exemple de l'ensemble de donn√©es *medic* pour voir sous quelle forme se pr√©sentent les donn√©es :


```python
print(dataset["train"][0])
```

    {'audio': 'data/audio/156.wav', 'sentence': "L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide."}


Nous pouvons voir que nous avons un tableau audio d'entr√©e √† une dimension et la transcription cible correspondante. Nous avons beaucoup parl√© de l'importance du taux d'√©chantillonnage et du fait que nous devons faire correspondre le taux d'√©chantillonnage de notre audio √† celui du mod√®le Whisper (16kHz). Si notre audio d'entr√©e √©tait √©chantillonn√© √† 48kHz ou autre chose que 16Khz, nous devrions le re-√©chantillonner √† 16kHz avant de le passer √† l'extracteur de caract√©ristiques de Whisper.

Nous allons r√©gler les entr√©es audio sur la fr√©quence d'√©chantillonnage correcte √† l'aide de la m√©thode cast_column du jeu de donn√©es. Cette op√©ration ne modifie pas l'audio sur place, mais signale aux datasets de r√©√©chantillonner les √©chantillons audio √† la vol√©e la premi√®re fois qu'ils sont charg√©s :


```python
from datasets import Audio

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

Le rechargement du premier √©chantillon audio dans l'ensemble de donn√©es *Medic* le r√©√©chantillonnera √† la fr√©quence d'√©chantillonnage souhait√©e :


```python
print(dataset["train"][0])
```

    {'audio': {'path': 'data/audio/156.wav', 'array': array([0.00082397, 0.00024414, 0.        , ..., 0.00033569, 0.00024414,
           0.00024414]), 'sampling_rate': 16000}, 'sentence': "L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide."}


Les valeurs du tableau peuvent √™tre diff√©rentes.

Nous pouvons maintenant √©crire une fonction pour pr√©parer nos donn√©es pour le mod√®le :


1.   Nous chargeons et r√©√©chantillonnons les donn√©es audio en appelant batch[¬´ audio ¬ª]. Comme expliqu√© ci-dessus, ü§ó Datasets effectue toutes les op√©rations de r√©√©chantillonnage n√©cessaires √† la vol√©e.
2.   Nous utilisons l'extracteur de caract√©ristiques pour calculer les caract√©ristiques d'entr√©e du spectrogramme log-Mel √† partir de notre tableau audio unidimensionnel.
3.   Nous codons les transcriptions en identifiants d'√©tiquettes √† l'aide du tokenizer.


```python
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```

Nous pouvons appliquer la fonction de pr√©paration des donn√©es √† tous nos exemples d'apprentissage en utilisant la m√©thode .map du jeu de donn√©es :


```python
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names["train"], num_proc=4)
```
```
Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:06<00:00, 29.36 examples/s]
Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:01<00:00, 26.53 examples/s]
```

Tr√®s bien ! Avec cela, nous avons nos donn√©es enti√®rement pr√©par√©es pour l'entra√Ænement ! Continuons et regardons comment nous pouvons utiliser ces donn√©es pour affiner Whisper.

> Note : Actuellement, les jeux de donn√©es utilisent √† la fois torchaudio et librosa pour le chargement et le r√©√©chantillonnage audio. Si vous souhaitez mettre en ≈ìuvre votre propre chargement/√©chantillonnage de donn√©es, vous pouvez utiliser la colonne ¬´ path ¬ª pour obtenir le chemin du fichier audio et ignorer la colonne ¬´ audio ¬ª.

## Training et Evaluation
Maintenant que nous avons pr√©par√© nos donn√©es, nous sommes pr√™ts √† nous plonger dans le pipeline de formation. Le ü§ó Trainer va faire le gros du travail √† notre place. Tout ce que nous avons √† faire, c'est :

*   Charger un point de contr√¥le pr√©-entra√Æn√© : nous devons charger un point de contr√¥le pr√©-entra√Æn√© et le configurer correctement pour l'entra√Ænement.
*   D√©finir un collateur de donn√©es : le collateur de donn√©es prend nos donn√©es pr√©trait√©es et pr√©pare des tenseurs PyTorch pr√™ts pour le mod√®le.
* M√©triques d'√©valuation : lors de l'√©valuation, nous voulons √©valuer le mod√®le √† l'aide de la m√©trique du taux d'erreur sur les mots (WER). Nous devons d√©finir une fonction compute_metrics qui g√®re ce calcul.
* D√©finir les arguments de formation : ils seront utilis√©s par le ü§ó *Trainer* pour construire le programme de formation.

Une fois le mod√®le affin√©, nous l'√©valuerons sur les donn√©es de test afin de v√©rifier que nous l'avons correctement entra√Æn√© √† transcrire des termes m√©dicaux.

## Charger un point de contr√¥le pr√©-entra√Æn√© (Pre-Trained Checkpoint)
Nous commencerons notre cycle de r√©glage fin √† partir du point de contr√¥le pr√©-entra√Æn√© de Whisper small. Pour ce faire, nous chargerons les poids pr√©-entra√Æn√©s du Hugging Face Hub. Encore une fois, cette op√©ration est triviale gr√¢ce √† l'utilisation de ü§ó Transformers !


```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(model_name)
```

Au moment de l'inf√©rence, le mod√®le Whisper d√©tecte automatiquement la langue de l'audio source et pr√©dit les identifiants de jetons dans cette langue. Dans les cas o√π la langue de l'audio source est connue a-priori, comme dans le cas d'un r√©glage fin multilingue, il est avantageux de d√©finir la langue de mani√®re explicite. Cela permet d'√©viter les sc√©narios dans lesquels la langue incorrecte est pr√©dite, ce qui entra√Æne une divergence entre le texte pr√©dit et la langue r√©elle au cours de la g√©n√©ration. Pour ce faire, nous d√©finissons les arguments *language* et *task* dans la configuration de la g√©n√©ration.


```python
model.generation_config.language = "french"
model.generation_config.task = "transcribe"
```

## D√©finir un collecteur de donn√©es (DataCollator)
Le collecteur de donn√©es pour un mod√®le vocal s√©quence √† s√©quence est unique en ce sens qu'il traite les *input_features* et *labels* ind√©pendamment : les caract√©ristiques d'entr√©e (*input_features*) doivent √™tre trait√©es par l'extracteur de caract√©ristiques (*extractor*) et les √©tiquettes (*labels*) par le tokenizer.

Les *input_features* ont d√©j√† √©t√© ramen√©s √† 30 secondes et converties en un spectrogramme log-Mel de dimension fixe, de sorte qu'il ne nous reste plus qu'√† les convertir en tenseurs PyTorch en lots. Nous le faisons en utilisant la m√©thode .pad de l'extractor avec return_tensors=pt. Notez qu'aucun rembourrage suppl√©mentaire (*additional padding*)n'est appliqu√© ici puisque les entr√©es sont de dimension fixe, les *input_features* sont simplement convertis en tenseurs PyTorch.

En revanche, les *labels* ne sont pas tamponn√©s (*un-padded*). Nous commen√ßons par ajouter un *pad* aux s√©quences jusqu'√† la longueur maximale du lot √† l'aide de la m√©thode .pad du tokenizer. Les jetons de remplissage (*padding tokkens*)sont ensuite remplac√©s par -100 afin que ces jetons ne soient pas pris en compte lors du calcul de la perte. Nous coupons ensuite le d√©but du jeton de transcription du d√©but de la s√©quence d'√©tiquettes, car nous l'ajouterons plus tard au cours de la formation.

Nous pouvons nous appuyer sur le **WhisperProcessor** que nous avons d√©fini pr√©c√©demment pour effectuer les op√©rations d'extraction de caract√©ristiques et de symbolisation :


```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # diviser les entr√©es et les √©tiquettes, car elles doivent √™tre de longueurs diff√©rentes et n√©cessitent des m√©thodes de remplissage diff√©rentes
        # traite d'abord les entr√©es audio en renvoyant simplement des tenseurs de torch
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # Obtenir les s√©quences d'√©tiquettes symbolis√©es
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # remplir les labels jusqu'√† la longueur maximale
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # remplacer le remplissage par -100 pour ignorer correctement la perte
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # si le jeton de d√©but est ajout√© lors de l'√©tape pr√©c√©dente
        # le retirer ici puisqu'il sera de toute fa√ßon ajout√© plus tard
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        # d√©finir le masque d'attention
        # cr√©er un masque d'attention
        attention_mask = torch.ones(batch["input_features"].shape, dtype=torch.long)
        attention_mask[batch["input_features"] == 0] = 0

        batch["attention_mask"] = attention_mask
        batch["labels"] = labels

        return batch
```

Initialisons le collecteur de donn√©es que nous venons de d√©finir :


```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)
```

## Evaluations

Apr√®s l'entra√Ænement, nous √©valuons les performances du mod√®le avec le **Word Error Rate (WER)**, qui est la m√©trique standard pour √©valuer les syst√®mes de reconnaissance vocale (ASR). Le WER calcule le pourcentage d'erreurs dans la transcription, en comparant le texte de sortie avec une r√©f√©rence correcte. Nous chargeons la m√©trique WER via ü§ó **Evaluate**. Pour plus de d√©tails sur le calcul du WER, vous pouvez consulter la [documentation](https://huggingface.co/metrics/wer).



```python
import evaluate

metric = evaluate.load("wer")
```

Il suffit ensuite de d√©finir une fonction qui prend les pr√©dictions de notre mod√®le et renvoie la m√©trique WER. Cette fonction, appel√©e compute_metrics, remplace d'abord -100 par pad_token_id dans les labels_ids (annulant l'√©tape que nous avons appliqu√©e dans le data collator pour ignorer correctement les tokens padded dans la perte). Il d√©code ensuite les identifiants pr√©dits et d'√©tiquettes en cha√Ænes de caract√®res. Enfin, il calcule le WER entre les pr√©dictions et les √©tiquettes de r√©f√©rence :


```python
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # remplacer -100 par l'identifiant du jeton de remplissage
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # nous ne voulons pas regrouper les jetons lors du calcul des m√©triques
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}
```

## D√©finir les arguments de formation (Training Arguments)

Dans la derni√®re √©tape, nous d√©finissons tous les param√®tres li√©s √† la formation. Un sous-ensemble de param√®tres est expliqu√© ci-dessous :


* *output_dir* : r√©pertoire local dans lequel enregistrer les poids du mod√®le.
* *generation_max_length* : nombre maximal de tokens √† g√©n√©rer de mani√®re autor√©gressive pendant l'√©valuation.
* *save_steps* : pendant la formation, les points de contr√¥le interm√©diaires seront enregistr√©s et t√©l√©charg√©s de mani√®re asynchrone vers le Hub tous les save_steps pas de formation.
* *eval_steps* : pendant la formation, l'√©valuation des points de contr√¥le interm√©diaires sera effectu√©e tous les eval_steps pas de formation.
* *report_to* : o√π enregistrer les journaux de formation. Les plateformes support√©es sont ¬´ azure_ml ¬ª, ¬´ comet_ml ¬ª, ¬´ mlflow ¬ª, ¬´ neptune ¬ª, ¬´ tensorboard ¬ª et ¬´ wandb ¬ª. Choisissez votre plateforme pr√©f√©r√©e ou laissez ¬´ tensorboard ¬ª. Nous avons utilis√© **Tensorboard**

Pour plus de d√©tails sur les autres arguments d'entra√Ænement, consultez la [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments) Seq2SeqTrainingArguments.


```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-base-french-medic",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # augmenter par 2x pour chaque r√©duction de 2x de la taille de lot (batch_size)
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=700, #5000
    gradient_checkpointing=True,
    fp16=True,
    eval_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=25,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    run_name="essais_base",
    logging_dir="./whisper-base-french-medic/logs/essais_base"
)
```

Nous pouvons transmettre les arguments d'entra√Ænement au ü§ó Trainer avec notre mod√®le, notre jeu de donn√©es, notre collecteur de donn√©es et notre fonction compute_metrics :

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)
```
```
    max_steps is given, it will override any value given in num_train_epochs
```

Et voil√†, nous sommes pr√™ts √† commencer l'entra√Ænement !

## Formation (Training)

Pour lancer une formation, il suffit d'ex√©cuter :


```python
torch.utils.checkpoint.use_reentrant = False
trainer.train()
```
```
    {'train_runtime': 20297.6998, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.034, 'train_loss': 0.181169177887163, 'epoch': 58.33}
    TrainOutput(global_step=700, training_loss=0.181169177887163, metrics={'train_runtime': 20297.6998, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.034, 'total_flos': 6.8128939966464e+17, 'train_loss': 0.181169177887163, 'epoch': 58.333333333333336})
```

L'entra√Ænement a dur√© 5h37, et peut varier en fonction du mod√®le de base, de l'utilisation ou non d'un GPU ou de celui allou√© au Google Colab si vous l'effectuez avec. Il est possible que vous rencontriez une erreur CUDA ¬´ out-of-memory ¬ª lorsque vous commencez l'entra√Ænement. Dans ce cas, vous pouvez r√©duire la taille du lot  (per_device_train_batch_size) par incr√©ments d'un facteur 2 et utiliser les √©tapes d'accumulation du gradient (gradient_accumulation_steps) pour compenser.

On donne le nom de notre mod√®le entra√Æn√© et on le sauvegarde


```python
finetuned_directory = "./whisper-base-ch-perigueux"
model.save_pretrained(finetuned_directory)
processor.save_pretrained(finetuned_directory)
```
```
    Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
    Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}
```
## 7. Adaptation pour whisperLive

Pour la reconnaissance vocale en temps r√©el, **faster-whisper** est une variante optimis√©e de **Whisper** con√ßue pour des performances plus rapides avec un compromis sur l‚Äôutilisation de ressources. Les mod√®les **Whisper** standards sont plus lourds, mais offrent une plus grande pr√©cision en traitement par lot. **Faster-whisper** utilise **CTranslate2**, une biblioth√®que sp√©cialis√©e pour l‚Äôinf√©rence rapide, particuli√®rement utile en temps r√©el ou sur des appareils √† faible puissance. 

Pour passer un mod√®le **Whisper** en **faster-whisper**, il faut convertir le mod√®le en utilisant **CTranslate2** et r√©cup√©rer le fichier **tokenizer.json** du mod√®le pr√©alablement fine-tun√©, afin de maintenir la coh√©rence du vocabulaire lors de l'inf√©rence.

```python
import os
import tokenizers

tokenizer_file = os.path.join(finetuned_directory, "tokenizer.json")
print(f"save  {tokenizer_file}", end="")
hf_tokenizer = tokenizers.Tokenizer.from_pretrained(model_name)
# enregistrer le tokenizer dans un fichier dans le r√©pertoire sp√©cifi√©
hf_tokenizer.save(tokenizer_file)
print(" Ok")
```
```
    save  ./whisper-base-ch-perigueux/tokenizer.json Ok
```

```python
import shutil
import ctranslate2

# Chemin vers le dossier o√π le mod√®le est sauvegard√© dans le format
output_dir = f"{finetuned_directory}-faster-whisper"

# Valeurs possibles pour quantization:

#     "int8" : Quantification en 8 bits pour r√©duire la taille du mod√®le, souvent utilis√© pour les inf√©rences sur CPU.
#     "int16" : Quantification en 16 bits, un compromis entre la vitesse et la pr√©cision.
#     "float16" : Utilis√© pour les inf√©rences sur GPU, o√π les calculs peuvent √™tre effectu√©s en 16 bits flottants.
quantization = "int16"

# Convertir le mod√®le en CTranslate2
converter = ctranslate2.converters.TransformersConverter(finetuned_directory)
converter.convert(output_dir, quantization=quantization, force=True)

print(f"Le mod√®le a √©t√© converti et sauvegard√© dans {output_dir}")

shutil.copy(tokenizer_file, output_dir)
print(f"Fichier {tokenizer_file} copi√© dans {output_dir}")
```
```
    Le mod√®le a √©t√© converti et sauvegard√© dans ./whisper-base-ch-perigueux-faster-whisper
    Fichier ./whisper-base-ch-perigueux/tokenizer.json copi√© dans ./whisper-base-ch-perigueux-faster-whisper
```
## 8. R√©sultats

Les graphiques g√©n√©r√©s par TensorBoard montrent les performances du mod√®le au cours de l'entra√Ænement :

|![Training](images/Progressions.png)


1. **Perte (eval/loss)** : La perte diminue r√©guli√®rement, atteignant environ 0,28, signe de la convergence du mod√®le. Cette stabilisation apr√®s 600 √©tapes indique une bonne optimisation.

2. **WER (Word Error Rate)** : Le WER suit une baisse significative, passant de 35 % √† environ 15 %, montrant que le mod√®le am√©liore la transcription.

3. **Taux d'apprentissage** : Le taux d'apprentissage suit un sch√©ma de "warm-up" puis d√©cro√Æt. Cette approche, classique en optimisation, aide √† √©viter l'instabilit√© initiale et facilite la convergence vers une solution optimale. 

L'ensemble des graphiques d√©montre une progression positive avec une bonne stabilit√© du mod√®le apr√®s plusieurs it√©rations.

### Calcul du WER sur les donn√©es audio test:
Le **WER** calcul√© avant et apr√®s l'entra√Ænement, sur des audios tests qui n'ont pas √©t√© '*entendus*' par les deux mod√®les, passe de **64,01%** √† **34,64%.**

![R√©sultats](images/Resultat.png)


## 9. Conclusion

Le fine-tuning de Whisper pour la transcription des termes m√©dicaux en fran√ßais d√©montre qu'il est possible d'am√©liorer les performances m√™me avec un ensemble de donn√©es g√©n√©r√© artificiellement. En utilisant des technologies comme Mistral et Bark, nous avons pu g√©n√©rer des donn√©es vocales m√©dicales et fine-tuner le mod√®le de mani√®re efficace. Le WER a √©t√© r√©duit de mani√®re significative avec 180 petits rapports m√©dicaux, rendant cette approche prometteuse.

## 10. R√©f√©rences

- Radford, Alec, et al. (2022). "Whisper: Multilingual and multitask speech recognition model." OpenAI.
- [Hugging Face - Fine-tune Whisper](https://huggingface.co/blog/fine-tune-whisper)
- [Hugging Face - Word Error Rate (WER)](https://huggingface.co/metrics/wer)
- Google AI. (2019). "SpecAugment: A New Data Augmentation Method for Speech Recognition." [Google Blog](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)
- [GitHub Repository - Finetuning Whisper](https://github.com/sirius911/Finetuning)
