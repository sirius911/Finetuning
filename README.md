# **Fine-Tune de Whisper Pour des termes m√©dicaux en fran√ßais**

Ecrit √† partir de [https://huggingface.co/blog/fine-tune-whisper](https://huggingface.co/blog/fine-tune-whisper)

## Introduction

Whisper est un mod√®le pr√©-entra√Æn√© pour la reconnaissance automatique de la parole (ASR) publi√© en septembre 2022 par les auteurs Alec Radford et al. de l'OpenAI. Contrairement √† nombre de ses pr√©d√©cesseurs, tels que Wav2Vec 2.0, qui sont pr√©-entra√Æn√©s sur des donn√©es audio non √©tiquet√©es, Whisper est pr√©-entra√Æn√© sur une grande quantit√© de donn√©es de transcription audio √©tiquet√©es, 680 000 heures pour √™tre pr√©cis. Il s'agit d'un ordre de grandeur plus important que les donn√©es audio non √©tiquet√©es utilis√©es pour entra√Æner Wav2Vec 2.0 (60 000 heures). De plus, 117 000 heures de ces donn√©es de pr√©-entra√Ænement sont des donn√©es ASR multilingues. Il en r√©sulte des checkpoints qui peuvent √™tre appliqu√©s √† plus de 96 langues, dont beaucoup sont consid√©r√©es comme des langues √† faibles ressources.

Cette quantit√© de donn√©es √©tiquet√©es permet √† Whisper d'√™tre pr√©-entra√Æn√© directement sur la t√¢che supervis√©e de reconnaissance de la parole, en apprenant une correspondance parole-texte √† partir des donn√©es de pr√©-entra√Ænement audio-transcription √©tiquet√©es 11. Par cons√©quent, Whisper ne n√©cessite que peu d'ajustements suppl√©mentaires pour produire un mod√®le ASR performant. Contrairement √† Wav2Vec 2.0, qui est pr√©-entra√Æn√© sur la t√¢che non supervis√©e de la pr√©diction masqu√©e. Ici, le mod√®le est form√© pour apprendre une correspondance interm√©diaire entre la parole et les √©tats cach√©s √† partir de donn√©es audio non √©tiquet√©es. Bien que le pr√©-entra√Ænement non supervis√© produise des repr√©sentations de haute qualit√© de la parole, il ne permet pas d'apprendre une correspondance parole-texte. Cette correspondance n'est apprise qu'au cours du r√©glage fin, ce qui n√©cessite un r√©glage plus fin pour obtenir des performances comp√©titives.

Lorsqu'ils sont mis √† l'√©chelle avec 680 000 heures de donn√©es de pr√©-entra√Ænement √©tiquet√©es, les mod√®les Whisper d√©montrent une forte capacit√© √† se g√©n√©raliser √† de nombreux ensembles de donn√©es et domaines. Les points de contr√¥le pr√©-entra√Æn√©s obtiennent des r√©sultats comp√©titifs par rapport aux syst√®mes ASR de pointe, avec un taux d'erreur de mot (WER) proche de 3% sur le sous-ensemble test-propre de LibriSpeech ASR et un nouvel √©tat de l'art sur TED-LIUM avec un WER de 4,7% (cf. tableau 8 de l'article sur Whisper). La connaissance approfondie de l'ASR multilingue acquise par Whisper pendant le pr√©-entra√Ænement peut √™tre exploit√©e pour d'autres langues √† faibles ressources ; gr√¢ce √† un r√©glage fin, les points de contr√¥le pr√©-entra√Æn√©s peuvent √™tre adapt√©s √† des ensembles de donn√©es et √† des langues sp√©cifiques afin d'am√©liorer encore ces r√©sultats.

Whisper est un mod√®le de codage-d√©codage bas√© sur un transformateur, √©galement appel√© mod√®le s√©quence-s√©quence. Il √©tablit une correspondance entre une s√©quence de caract√©ristiques de spectrogrammes audio et une s√©quence de mots-cl√©s de texte. Tout d'abord, les entr√©es audio brutes sont converties en un spectrogramme log-Mel par l'action de l'extracteur de caract√©ristiques. L'encodeur Transformer encode ensuite le spectrogramme pour former une s√©quence d'√©tats cach√©s de l'encodeur. Enfin, le d√©codeur pr√©dit de mani√®re autor√©gressive les mots-cl√©s du texte, en fonction des mots-cl√©s pr√©c√©dents et des √©tats cach√©s du codeur.

La figure 1 r√©sume le mod√®le Whisper.

![mod√®le Whisper](images/whisper_architecture.jpg "figure 1")


Figure 1 : Mod√®le Whisper. L'architecture suit le mod√®le standard de codeur-d√©codeur bas√© sur un transformateur. Un spectrogramme log-Mel est introduit dans le codeur. Les derniers √©tats cach√©s du codeur sont transmis au d√©codeur via des m√©canismes d'attention crois√©e. Le d√©codeur pr√©dit de mani√®re autor√©gressive les mots-cl√©s du texte, en fonction des √©tats cach√©s du codeur et des mots-cl√©s pr√©dits pr√©c√©demment. Source de la figure : OpenAI Whisper Blog.

Dans un mod√®le s√©quence √† s√©quence, le codeur transforme les entr√©es audio en un ensemble de repr√©sentations d'√©tats cach√©s, en extrayant les caract√©ristiques importantes de la parole. Le d√©codeur joue le r√¥le d'un mod√®le linguistique, traitant les repr√©sentations d'√©tats cach√©s et g√©n√©rant les transcriptions textuelles correspondantes. L'incorporation d'un mod√®le linguistique en interne dans l'architecture du syst√®me est appel√©e fusion profonde. Cela contraste avec la fusion superficielle, o√π un mod√®le linguistique est combin√© de mani√®re externe avec un codeur, comme avec CTC + nn-gram (c.f. Internal Language Model Estimation). Avec la fusion profonde, l'ensemble du syst√®me peut √™tre entra√Æn√© de bout en bout avec les m√™mes donn√©es d'entra√Ænement et la m√™me fonction de perte, ce qui offre une plus grande flexibilit√© et des performances g√©n√©ralement sup√©rieures (c.f. ESB Benchmark).

Whisper est pr√©-entra√Æn√© et ajust√© en utilisant la fonction objective de l'entropie crois√©e, une fonction objective standard pour l'entra√Ænement des syst√®mes s√©quence-√†-s√©quence sur des t√¢ches de classification. Ici, le syst√®me est entra√Æn√© √† classer correctement le jeton de texte cible √† partir d'un vocabulaire pr√©d√©fini de jetons de texte.

Les versions de mod√®les Whisper(checkpoint) sont disponibles en cinq configurations de tailles de mod√®les diff√©rentes. Les quatre plus petits sont entra√Æn√©s sur des donn√©es en anglais uniquement ou multilingues. Les checkpoints les plus grands sont uniquement multilingues. Les 11 checkpoints pr√©-entra√Æn√©s sont disponibles sur le Hugging Face Hub. Les checkpoints sont r√©sum√©s dans le tableau suivant avec des liens vers les mod√®les sur le Hub :

| Size     | Layers | Width | Heads | Parameters | English-only | Multilingual |
|----------|--------|-------|-------|------------|--------------|--------------|
| tiny     | 4      | 384   | 6     | 39 M       | ‚úì            | ‚úì            |
| base     | 6      | 512   | 8     | 74 M       | ‚úì            | ‚úì            |
| small    | 12     | 768   | 12    | 244 M      | ‚úì            | ‚úì            |
| medium   | 24     | 1024  | 16    | 769 M      | ‚úì            | ‚úì            |
| large    | 32     | 1280  | 20    | 1550 M     | x            | ‚úì            |
| large-v2 | 32     | 1280  | 20    | 1550 M     | x            | ‚úì            |
| large-v3 | 32     | 1280  | 20    | 1550 M     | x            | ‚úì            |


## Pr√©parer l'environnement

Voici la liste des modules Python √† installer pour pr√©parer l'environnement :

1. **`datasets[audio]`** : pour t√©l√©charger et pr√©parer les donn√©es d'entra√Ænement audio.
2. **`transformers`** : pour charger et entra√Æner le mod√®le Whisper.
3. **`accelerate`** : pour acc√©l√©rer l'entra√Ænement du mod√®le.
4. **`evaluate`** : pour √©valuer les performances avec des m√©triques.
5. **`jiwer`** : pour calculer le taux d'erreur de mots (WER).
6. **`tensorboard`** : pour suivre les m√©triques d'entra√Ænement.
7. **`soundfile`** : pour pr√©traiter les fichiers audio.
8. **`pandas`** : pour manipuler des DataFrames lors de la cr√©ation des fichiers CSV d'entra√Ænement et de test.
9. **`sklearn`** : pour diviser les donn√©es en ensembles d'entra√Ænement et de test.
10. **`torch`** : pour g√©rer les tenseurs et entra√Æner le mod√®le Whisper.
11. **`ctranslate2`** : pour convertir le mod√®le en format compatible avec `faster-whisper`.

Cela devrait couvrir tous les modules n√©cessaires pour ex√©cuter les exemples et le code fournis dans le fichier README.
```sh
pip install --upgrade pip
pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard soundfile pandas scikit-learn torch ctranslate2

```
L'utilisation d'un environnement python est conseill√©.(dans les sources il y a aussi les requirements.txt utilis√©s pour l'exp√©rience)
Nous vous conseillons vivement de t√©l√©charger les checkpoints du mod√®le directement sur le Hugging Face Hub pendant l'entra√Ænement. Au moins pour le premier entrainement. Par la suite on peut mettre l'adresse physique du mod√®le d√©j√† t√©l√©charg√© ou le mod√®le personnel auparavant entra√Æn√©.

### Pr√©paration des datas

Nous avons besoins de fichiers audio et de leur transcriptions correctes.
Pour cel√†, nous avons mis dans un dossier data/ un dossier audio/, comprenant X fichiers .wav nomm√©s par un nombre. Dans le repertoire rapports/, nous avons mis la correspondance √©crite avec le m√™me numero en format txt (unicode UTF-8)
Nous verrons plus bas que les audios seront entrain√©s dans un format particuli√©. Les avoir dans ce format d√®s le d√©part peut faire gagner du temps.

Une fois ces fichiers r√©partis dans leurs dossiers, nous avons a faire deux fichiers (train.csv et test.csv) qui vont r√©partir ces donn√©es entre les donn√©es d'entra√Ænement et les donn√©es de test.
Voici un exmple de code faisant cela:


```python
import os
import pandas as pd
from sklearn.model_selection import train_test_split

# Define the paths
audio_dir = 'data/audio'
text_dir = 'data/rapports'

# Create a list to hold the data
data = []

# Loop over each file in the audio directory
for audio_file in sorted(os.listdir(audio_dir)):
    if audio_file.endswith('.wav'):  # Assuming the audio files are in .wav format
        # Construct the corresponding text file path
        text_file = os.path.splitext(audio_file)[0] + '.txt'
        text_file_path = os.path.join(text_dir, text_file)
        
        # Read the transcription text if the file exists
        if os.path.exists(text_file_path):
            with open(text_file_path, 'r', encoding='utf-8') as f:
                transcription = f.read().strip()
        
            # Add the information to the data list
            data.append({
                'audio': os.path.join(audio_dir, audio_file),
                'sentence': transcription
            })

# Convert the list to a DataFrame
df = pd.DataFrame(data)

# Split the data into 80% train and 20% test
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Save the DataFrame to separate CSV files for train and test
train_df.to_csv('train.csv', index=False)
test_df.to_csv('test.csv', index=False)

print("Train and Test CSV files created successfully.")

```
```
Train and Test CSV files created successfully.
```

Nous avons r√©parti l'ensemble de nos datas (226) en 80% (180) pour l'entrainement et 20%(46) pour le test.

Nous nous retrouvons donc avec des fichiers csv de la forme : 

nom_du_fichier_audio.wav, '*transcription*'


``` csv
audio,sentence
../data/audio/151.wav,"Le rapport biologique r√©v√®le une thyro√Ødite de Hashimoto caract√©ris√©e par une infiltration lymphocytaire et une fibrose diffuse, confirmant une pathologie auto-immune."
```

### Note sur la validit√© des donn√©es

Dans le principe, nous aurions besoins de v√©ritable rapports m√©dicaux avec les voix des personnels m√©dicaux, les dictants. N'ayant pas encore cela, nous avons eu l'id√©e de faire faire nos "rapports" m√©dicaux par un LLM (Mistral) et de les faire lire par le TextToSpeech BARK. Biensur la pertinence des textes m√©dicaux issue de Mistral, peut √™tre sujet √† interrogation, mais rappelons que notre but est de faire reconnaitre des termes m√©dicaux, et donc de faire "entendre" au mod√®le Whisper ces mots dans des contextes vari√©s. Notre objectif est alors de montrer que le mod√®le s'am√©liore en ayant entendu ces mots. Passer √† des textes r√©els et des voix humaines, ne peut qu'am√©liorer le mod√®le, tant dans par les contextes m√©dicaux, plus "r√©els" que par les voix humaine.



####¬†Charger l'ensemble de donn√©es (**Dataset**)


```python
from datasets import load_dataset, DatasetDict

# Load the local CSV files for train and test sets
data_files = {
    "train": "data/train.csv",  # Local path to train.csv
    "test": "data/test.csv"     # Local path to test.csv
}

# Load the dataset from the local CSV files
dataset = DatasetDict()
dataset["train"] = load_dataset("csv", data_files={"train": data_files["train"]}, split="train")
dataset["test"] = load_dataset("csv", data_files={"test": data_files["test"]}, split="test")

print(dataset)
```
```
DatasetDict({
    train: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 180
    })
    test: Dataset({
        features: ['audio', 'sentence'],
        num_rows: 46
    })
})
```
Notre version mod√®le (checkpoint) est celui de base d'openAI 


```python
import torch
from transformers import WhisperForConditionalGeneration, WhisperTokenizer

model_name = "openai/whisper-base"
model = WhisperForConditionalGeneration.from_pretrained(model_name)
tokenizer = WhisperTokenizer.from_pretrained(model_name)
```

## Pr√©parer l'extracteur de caract√©ristiques (Extractor), le tokenizer et les datas

Le pipeline ASR peut √™tre d√©compos√© en trois √©l√©ments :


1.   Un extracteur de caract√©ristiques qui pr√©-traite les entr√©es audio brutes
2.   Le mod√®le qui effectue le mappage s√©quence-s√©quence
3.   Un tokenizer qui post-traite les sorties du mod√®le au format texte.

Dans ü§ó Transformers, le mod√®le Whisper est associ√© √† un extracteur de caract√©ristiques et √† un tokenizer, appel√©s respectivement WhisperFeatureExtractor et WhisperTokenizer.

Nous allons d√©tailler l'extracteur de caract√©ristiques et le tokenizer un par un !

**Charger l'extracteur de caract√©ristiques WhisperFeatureExtractor**

La parole est repr√©sent√©e par un tableau √† une dimension qui varie avec le temps. La valeur du tableau √† un pas de temps donn√© est l'amplitude du signal √† ce moment-l√†. √Ä partir des seules informations sur l'amplitude, nous pouvons reconstruire le spectre de fr√©quences de l'audio et r√©cup√©rer toutes les caract√©ristiques acoustiques.

La parole √©tant continue, elle contient un nombre infini de valeurs d'amplitude. Cela pose des probl√®mes pour les appareils informatiques qui s'attendent √† des tableaux finis. Nous discr√©tisons donc notre signal vocal en √©chantillonnant des valeurs de notre signal √† des pas de temps fixes. L'intervalle avec lequel nous √©chantillonnons notre audio est connu sous le nom de taux d'√©chantillonnage et est g√©n√©ralement mesur√© en √©chantillons/sec ou en Hertz (Hz). L'√©chantillonnage avec un taux d'√©chantillonnage plus √©lev√© permet d'obtenir une meilleure approximation du signal vocal continu, mais n√©cessite √©galement le stockage de plus de valeurs par seconde.

Il est essentiel de faire correspondre la fr√©quence d'√©chantillonnage de nos entr√©es audio √† la fr√©quence d'√©chantillonnage attendue par notre mod√®le, car les signaux audio ayant des fr√©quences d'√©chantillonnage diff√©rentes ont des distributions tr√®s diff√©rentes. Les √©chantillons audio ne doivent √™tre trait√©s qu'avec la bonne fr√©quence d'√©chantillonnage. Le non-respect de cette r√®gle peut entra√Æner des r√©sultats inattendus ! Par exemple, si l'on prend un √©chantillon audio avec une fr√©quence d'√©chantillonnage de 16 kHz et qu'on l'√©coute avec une fr√©quence d'√©chantillonnage de 8 kHz, l'audio sonnera comme s'il √©tait en demi-vitesse. De la m√™me mani√®re, le passage d'un audio avec un taux d'√©chantillonnage incorrect peut faire √©chouer un mod√®le ASR qui s'attend √† un taux d'√©chantillonnage et en re√ßoit un autre. L'extracteur de caract√©ristiques Whisper attend des entr√©es audio avec un taux d'√©chantillonnage de 16kHz, nous devons donc faire correspondre nos entr√©es √† cette valeur. Nous ne voulons pas entra√Æner par inadvertance un syst√®me ASR sur de la parole au ralenti !

L'extracteur de caract√©ristiques Whisper effectue deux op√©rations. Tout d'abord, il compresse/tronque un lot d'√©chantillons audio de mani√®re √† ce que tous les √©chantillons aient une longueur d'entr√©e de 30 secondes. Les √©chantillons de moins de 30 secondes sont ramen√©s √† 30 secondes en ajoutant des z√©ros √† la fin de la s√©quence (les z√©ros dans un signal audio correspondent √† l'absence de signal ou au silence). Les √©chantillons de plus de 30 secondes sont tronqu√©s √† 30 secondes. √âtant donn√© que tous les √©l√©ments de la s√©rie sont compl√©t√©s/tronqu√©s √† une longueur maximale dans l'espace d'entr√©e, nous n'avons pas besoin d'un masque d'attention lorsque nous transmettons les entr√©es audio au mod√®le Whisper. Whisper est unique √† cet √©gard - avec la plupart des mod√®les audio, vous pouvez vous attendre √† fournir un masque d'attention qui d√©taille o√π les s√©quences ont √©t√© remplies, et donc o√π elles doivent √™tre ignor√©es dans le m√©canisme d'auto-attention. Whisper est entra√Æn√© √† fonctionner sans masque d'attention et √† d√©duire directement des signaux vocaux o√π ignorer les entr√©es.

La deuxi√®me op√©ration effectu√©e par l'extracteur de caract√©ristiques de Whisper consiste √† convertir les matrices audio en spectrogrammes log-Mel. Ces spectrogrammes sont une repr√©sentation visuelle des fr√©quences d'un signal, un peu comme une transform√©e de Fourier. Un exemple de spectrogramme est pr√©sent√© √† la figure 2. Le long de l'axe des ordonn√©es se trouvent les canaux Mel, qui correspondent √† des bins de fr√©quence particuliers. Le long de l'axe xx se trouve le temps. La couleur de chaque pixel correspond √† l'intensit√© logarithmique de ce groupe de fr√©quences √† un moment donn√©. Le spectrogramme log-Mel est la forme d'entr√©e attendue par le mod√®le Whisper.

Les canaux de Mel (bins de fr√©quence) sont standard dans le traitement de la parole et choisis pour se rapprocher de la gamme auditive humaine. Tout ce que nous avons besoin de savoir pour le r√©glage fin de Whisper, c'est que le spectrogramme est une repr√©sentation visuelle des fr√©quences du signal de parole. Pour plus de d√©tails sur les canaux Mel, voir le cepstre de fr√©quence Mel.

![spectogram](images/spectrogram.jpg)

Figure 2 : Conversion d'un r√©seau audio √©chantillonn√© en spectrogramme log-Mel. √Ä gauche : signal audio unidimensionnel √©chantillonn√©. √Ä droite : spectrogramme log-Mel correspondant. Source de la figure : [Google SpecAugment Blog.](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html)

Heureusement pour nous, l'extracteur de caract√©ristiques ü§ó Transformers Whisper effectue √† la fois le padding et la conversion du spectrogramme en une seule ligne de code ! Chargeons l'extracteur de caract√©ristiques √† partir du point de contr√¥le pr√©-entra√Æn√© pour qu'il soit pr√™t pour nos donn√©es audio :


```python
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)
```

## Load WhisperTokenizer
Voyons maintenant comment charger un tokenizer Whisper. Le mod√®le Whisper produit des jetons de texte qui indiquent l'index du texte pr√©dit dans le dictionnaire des √©l√©ments de vocabulaire. Le tokenizer fait correspondre une s√©quence de jetons de texte √† la cha√Æne de texte r√©elle (par exemple, [1169, 3797, 3332] -> ¬´ le chat s'est assis ¬ª).

Traditionnellement, lors de l'utilisation de mod√®les √† encodeur seul pour l'ASR, nous d√©codons en utilisant la classification temporelle connexionniste ([CTC](https://distill.pub/2017/ctc/)). Dans ce cas, nous devons former un tokenizer CTC pour chaque ensemble de donn√©es que nous utilisons. L'un des avantages de l'utilisation d'une architecture codeur-d√©codeur est que nous pouvons directement exploiter le tokenizer du mod√®le pr√©-entra√Æn√©.

Le tokenizer Whisper est pr√©-entra√Æn√© sur les transcriptions des 96 langues de pr√©-entra√Ænement. Par cons√©quent, il dispose d'une [paire d'octets](https://huggingface.co/course/chapter6/5?fw=pt#bytepair-encoding-tokenization) √©tendue qui convient √† presque toutes les applications ASR multilingues. Pour l'hindi, nous pouvons charger le tokenizer et l'utiliser pour un r√©glage fin sans aucune autre modification. Il suffit de sp√©cifier la langue cible et la t√¢che. Ces arguments indiquent au tokenizer de pr√©fixer les tokens de la langue et de la t√¢che au d√©but des s√©quences d'√©tiquettes encod√©es :


```python
from transformers import WhisperTokenizer

tokenizer = WhisperTokenizer.from_pretrained(model_name, language="French", task="transcribe")
```


> Conseil : l'article de blog peut √™tre adapt√© √† la traduction vocale en d√©finissant la t√¢che sur traduire ¬´*translate*¬ª et la langue sur la langue du texte cible dans la ligne ci-dessus. Cela permettra d'ajouter √† la t√¢che et √† la langue les tokens n√©cessaires √† la traduction vocale lors du pr√©traitement de l'ensemble de donn√©es.


Nous pouvons v√©rifier que le tokenizer encode correctement les caract√®res hindis en encodant et en d√©codant le premier √©chantillon de l'ensemble de donn√©es Common Voice. Lors de l'encodage des transcriptions, le tokenizer ajoute des ¬´ jetons sp√©ciaux ¬ª au d√©but et √† la fin de la s√©quence, y compris les jetons de d√©but/fin de transcription, le jeton de langue et les jetons de t√¢che (comme sp√©cifi√© par les arguments √† l'√©tape pr√©c√©dente). Lors du d√©codage des identifiants d'√©tiquettes, nous avons la possibilit√© de ¬´ sauter ¬ª ces jetons sp√©ciaux, ce qui nous permet de renvoyer une cha√Æne dans la forme d'entr√©e originale :


```python
input_str = dataset["train"][0]["sentence"]
labels = tokenizer(input_str).input_ids
decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)
decoded_str = tokenizer.decode(labels, skip_special_tokens=True)

print(f"Input:                 {input_str}")
print(f"Decoded w/ special:    {decoded_with_special}")
print(f"Decoded w/out special: {decoded_str}")
print(f"Are equal:             {input_str == decoded_str}")
```

    Input:                 L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.
    Decoded w/ special:    <|startoftranscript|><|fr|><|transcribe|><|notimestamps|>L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.<|endoftext|>
    Decoded w/out special: L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide.
    Are equal:             True


## Combiner pour cr√©er un WhisperProcessor

Pour simplifier l'utilisation de l'extracteur de caract√©ristiques et du tokenizer, nous pouvons les regrouper (*wrap*) dans une seule classe **WhisperProcessor**. Cet objet processeur **h√©rite** des classes **WhisperFeatureExtractor** et WhisperProcessor et peut √™tre utilis√© sur les entr√©es audio et les pr√©dictions du mod√®le selon les besoins. Ce faisant, nous n'avons besoin de suivre que deux objets pendant l'apprentissage : le processeur et le mod√®le :


```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained(model_name, language="French", task="transcribe")
```

**Pr√©parer les donn√©es**

Imprimons le premier exemple de l'ensemble de donn√©es *medic* pour voir sous quelle forme se pr√©sentent les donn√©es :


```python
print(dataset["train"][0])
```

    {'audio': 'data/audio/156.wav', 'sentence': "L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide."}


Nous pouvons voir que nous avons un tableau audio d'entr√©e √† une dimension et la transcription cible correspondante. Nous avons beaucoup parl√© de l'importance du taux d'√©chantillonnage et du fait que nous devons faire correspondre le taux d'√©chantillonnage de notre audio √† celui du mod√®le Whisper (16kHz). Si notre audio d'entr√©e √©tait √©chantillonn√© √† 48kHz ou autre chose que 16Khz, nous devrions le re-√©chantillonner √† 16kHz avant de le passer √† l'extracteur de caract√©ristiques de Whisper.

Nous allons r√©gler les entr√©es audio sur la fr√©quence d'√©chantillonnage correcte √† l'aide de la m√©thode cast_column du jeu de donn√©es. Cette op√©ration ne modifie pas l'audio sur place, mais signale aux datasets de r√©√©chantillonner les √©chantillons audio √† la vol√©e la premi√®re fois qu'ils sont charg√©s :


```python
from datasets import Audio

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

Le rechargement du premier √©chantillon audio dans l'ensemble de donn√©es *Medic* le r√©√©chantillonnera √† la fr√©quence d'√©chantillonnage souhait√©e :


```python
print(dataset["train"][0])
```

    {'audio': {'path': 'data/audio/156.wav', 'array': array([0.00082397, 0.00024414, 0.        , ..., 0.00033569, 0.00024414,
           0.00024414]), 'sampling_rate': 16000}, 'sentence': "L'arthroplastie c√©phalique a √©t√© r√©alis√©e avec succ√®s, sans complications perop√©ratoires. La proth√®se a √©t√© parfaitement int√©gr√©e, garantissant une mobilit√© articulaire optimale et une r√©cup√©ration fonctionnelle rapide."}


Les valeurs du tableau peuvent √™tre diff√©rentes.

Nous pouvons maintenant √©crire une fonction pour pr√©parer nos donn√©es pour le mod√®le :


1.   Nous chargeons et r√©√©chantillonnons les donn√©es audio en appelant batch[¬´ audio ¬ª]. Comme expliqu√© ci-dessus, ü§ó Datasets effectue toutes les op√©rations de r√©√©chantillonnage n√©cessaires √† la vol√©e.
2.   Nous utilisons l'extracteur de caract√©ristiques pour calculer les caract√©ristiques d'entr√©e du spectrogramme log-Mel √† partir de notre tableau audio unidimensionnel.
3.   Nous codons les transcriptions en identifiants d'√©tiquettes √† l'aide du tokenizer.


```python
def prepare_dataset(batch):
    # load and resample audio data from 48 to 16kHz
    audio = batch["audio"]

    # compute log-Mel input features from input audio array
    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    # encode target text to label ids
    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch
```

Nous pouvons appliquer la fonction de pr√©paration des donn√©es √† tous nos exemples d'apprentissage en utilisant la m√©thode .map du jeu de donn√©es :


```python
dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names["train"], num_proc=4)
```
```
Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180/180 [00:06<00:00, 29.36 examples/s]
Map (num_proc=4): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46/46 [00:01<00:00, 26.53 examples/s]
```

Tr√®s bien ! Avec cela, nous avons nos donn√©es enti√®rement pr√©par√©es pour l'entra√Ænement ! Continuons et regardons comment nous pouvons utiliser ces donn√©es pour affiner Whisper.

> Note : Actuellement, les jeux de donn√©es utilisent √† la fois torchaudio et librosa pour le chargement et le r√©√©chantillonnage audio. Si vous souhaitez mettre en ≈ìuvre votre propre chargement/√©chantillonnage de donn√©es, vous pouvez utiliser la colonne ¬´ path ¬ª pour obtenir le chemin du fichier audio et ignorer la colonne ¬´ audio ¬ª.



## Training et Evaluation
Maintenant que nous avons pr√©par√© nos donn√©es, nous sommes pr√™ts √† nous plonger dans le pipeline de formation. Le ü§ó Trainer va faire le gros du travail √† notre place. Tout ce que nous avons √† faire, c'est :


*   Charger un point de contr√¥le pr√©-entra√Æn√© : nous devons charger un point de contr√¥le pr√©-entra√Æn√© et le configurer correctement pour l'entra√Ænement.
*   D√©finir un collateur de donn√©es : le collateur de donn√©es prend nos donn√©es pr√©trait√©es et pr√©pare des tenseurs PyTorch pr√™ts pour le mod√®le.
* M√©triques d'√©valuation : lors de l'√©valuation, nous voulons √©valuer le mod√®le √† l'aide de la m√©trique du taux d'erreur sur les mots (WER). Nous devons d√©finir une fonction compute_metrics qui g√®re ce calcul.
* D√©finir les arguments de formation : ils seront utilis√©s par le ü§ó *Trainer* pour construire le programme de formation.

Une fois le mod√®le affin√©, nous l'√©valuerons sur les donn√©es de test afin de v√©rifier que nous l'avons correctement entra√Æn√© √† transcrire des termes m√©dicaux.

## Charger un point de contr√¥le pr√©-entra√Æn√© (Pre-Trained Checkpoint)
Nous commencerons notre cycle de r√©glage fin √† partir du point de contr√¥le pr√©-entra√Æn√© de Whisper small. Pour ce faire, nous chargerons les poids pr√©-entra√Æn√©s du Hugging Face Hub. Encore une fois, cette op√©ration est triviale gr√¢ce √† l'utilisation de ü§ó Transformers !


```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained(model_name)
```

Au moment de l'inf√©rence, le mod√®le Whisper d√©tecte automatiquement la langue de l'audio source et pr√©dit les identifiants de jetons dans cette langue. Dans les cas o√π la langue de l'audio source est connue a-priori, comme dans le cas d'un r√©glage fin multilingue, il est avantageux de d√©finir la langue de mani√®re explicite. Cela permet d'√©viter les sc√©narios dans lesquels la langue incorrecte est pr√©dite, ce qui entra√Æne une divergence entre le texte pr√©dit et la langue r√©elle au cours de la g√©n√©ration. Pour ce faire, nous d√©finissons les arguments *language* et *task* dans la configuration de la g√©n√©ration.


```python
model.generation_config.language = "french"
model.generation_config.task = "transcribe"
```

## D√©finir un collecteur de donn√©es (DataCollator)
Le collecteur de donn√©es pour un mod√®le vocal s√©quence √† s√©quence est unique en ce sens qu'il traite les *input_features* et *labels* ind√©pendamment : les caract√©ristiques d'entr√©e (*input_features*) doivent √™tre trait√©es par l'extracteur de caract√©ristiques (*extractor*) et les √©tiquettes (*labels*) par le tokenizer.

Les *input_features* ont d√©j√† √©t√© ramen√©s √† 30 secondes et converties en un spectrogramme log-Mel de dimension fixe, de sorte qu'il ne nous reste plus qu'√† les convertir en tenseurs PyTorch en lots. Nous le faisons en utilisant la m√©thode .pad de l'extractor avec return_tensors=pt. Notez qu'aucun rembourrage suppl√©mentaire (*additional padding*)n'est appliqu√© ici puisque les entr√©es sont de dimension fixe, les *input_features* sont simplement convertis en tenseurs PyTorch.

En revanche, les *labels* ne sont pas tamponn√©s (*un-padded*). Nous commen√ßons par ajouter un *pad* aux s√©quences jusqu'√† la longueur maximale du lot √† l'aide de la m√©thode .pad du tokenizer. Les jetons de remplissage (*padding tokkens*)sont ensuite remplac√©s par -100 afin que ces jetons ne soient pas pris en compte lors du calcul de la perte. Nous coupons ensuite le d√©but du jeton de transcription du d√©but de la s√©quence d'√©tiquettes, car nous l'ajouterons plus tard au cours de la formation.

Nous pouvons nous appuyer sur le **WhisperProcessor** que nous avons d√©fini pr√©c√©demment pour effectuer les op√©rations d'extraction de caract√©ristiques et de symbolisation :


```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    decoder_start_token_id: int

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # diviser les entr√©es et les √©tiquettes, car elles doivent √™tre de longueurs diff√©rentes et n√©cessitent des m√©thodes de remplissage diff√©rentes
        # traite d'abord les entr√©es audio en renvoyant simplement des tenseurs de torch
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # Obtenir les s√©quences d'√©tiquettes symbolis√©es
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # pad the labels to max length
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # if bos token is appended in previous tokenization step,
        # cut bos token here as it's append later anyways
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        # Set the attention mask
        # create attention mask
        attention_mask = torch.ones(batch["input_features"].shape, dtype=torch.long)
        attention_mask[batch["input_features"] == 0] = 0

        batch["attention_mask"] = attention_mask
        batch["labels"] = labels

        return batch
```

Initialisons le collecteur de donn√©es que nous venons de d√©finir :


```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)
```

## M√©triques d'√©valuation

Ensuite, nous d√©finissons la m√©trique d'√©valuation que nous utiliserons sur notre ensemble d'√©valuation. Nous utiliserons le taux d'erreur de mots (WER), la m√©trique ¬´ de-facto ¬ª pour √©valuer les syst√®mes ASR. Pour plus d'informations, consultez la [documentation](https://huggingface.co/metrics/wer) sur le WER. Nous chargerons la m√©trique WER √† partir de ü§ó Evaluate :


```python
import evaluate

metric = evaluate.load("wer")
```

Il suffit ensuite de d√©finir une fonction qui prend les pr√©dictions de notre mod√®le et renvoie la m√©trique WER. Cette fonction, appel√©e compute_metrics, remplace d'abord -100 par pad_token_id dans les labels_ids (annulant l'√©tape que nous avons appliqu√©e dans le data collator pour ignorer correctement les tokens padded dans la perte). Il d√©code ensuite les identifiants pr√©dits et d'√©tiquettes en cha√Ænes de caract√®res. Enfin, il calcule le WER entre les pr√©dictions et les √©tiquettes de r√©f√©rence :


```python
def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # replace -100 with the pad_token_id
    label_ids[label_ids == -100] = tokenizer.pad_token_id

    # we do not want to group tokens when computing the metrics
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}
```

## D√©finir les arguments de formation (Training Arguments)

Dans la derni√®re √©tape, nous d√©finissons tous les param√®tres li√©s √† la formation. Un sous-ensemble de param√®tres est expliqu√© ci-dessous :


* *output_dir* : r√©pertoire local dans lequel enregistrer les poids du mod√®le. Ce sera √©galement le nom du d√©p√¥t sur le Hugging Face Hub.
* *generation_max_length* : nombre maximal de tokens √† g√©n√©rer de mani√®re autor√©gressive pendant l'√©valuation.
* *save_steps* : pendant la formation, les points de contr√¥le interm√©diaires seront enregistr√©s et t√©l√©charg√©s de mani√®re asynchrone vers le Hub tous les save_steps pas de formation.
* *eval_steps* : pendant la formation, l'√©valuation des points de contr√¥le interm√©diaires sera effectu√©e tous les eval_steps pas de formation.
* *report_to* : o√π enregistrer les journaux de formation. Les plateformes support√©es sont ¬´ azure_ml ¬ª, ¬´ comet_ml ¬ª, ¬´ mlflow ¬ª, ¬´ neptune ¬ª, ¬´ tensorboard ¬ª et ¬´ wandb ¬ª. Choisissez votre plateforme pr√©f√©r√©e ou laissez ¬´ tensorboard ¬ª pour vous connecter au Hub.

Pour plus de d√©tails sur les autres arguments d'entra√Ænement, consultez la [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments) Seq2SeqTrainingArguments.


```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-base-french-medic",  # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=700, #5000
    gradient_checkpointing=True,
    fp16=True,
    eval_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=1000,
    eval_steps=25,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,
    run_name="essais_base",
    logging_dir="./whisper-base-french-medic/logs/essais_base"
)
```

**Note** : si l'on ne souhaite pas t√©l√©charger les points de contr√¥le du mod√®le vers le Hub, d√©finir push_to_hub=False.

Nous pouvons transmettre les arguments d'entra√Ænement au ü§ó Trainer avec notre mod√®le, notre jeu de donn√©es, notre collecteur de donn√©es et notre fonction compute_metrics :


```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
)
```

    max_steps is given, it will override any value given in num_train_epochs


Et voil√†, nous sommes pr√™ts √† commencer l'entra√Ænement !

## Formation (Training)

Pour lancer une formation, il suffit d'ex√©cuter :


```python
torch.utils.checkpoint.use_reentrant = False
trainer.train()
```
    {'train_runtime': 20297.6998, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.034, 'train_loss': 0.181169177887163, 'epoch': 58.33}
    TrainOutput(global_step=700, training_loss=0.181169177887163, metrics={'train_runtime': 20297.6998, 'train_samples_per_second': 0.552, 'train_steps_per_second': 0.034, 'total_flos': 6.8128939966464e+17, 'train_loss': 0.181169177887163, 'epoch': 58.333333333333336})



L'entra√Ænement a dur√© 5h37, et peut varier en fonction du mod√®le de base, de l'utilisation ou non d'un GPU ou de celui allou√© au Google Colab si vous l'effectu√© avec. Il est possible que vous rencontriez une erreur CUDA ¬´ out-of-memory ¬ª lorsque vous commencez l'entra√Ænement. Dans ce cas, vous pouvez r√©duire la taille du lot  (per_device_train_batch_size) par incr√©ments d'un facteur 2 et utiliser les √©tapes d'accumulation du gradient (gradient_accumulation_steps) pour compenser.

On donne le nom de notre mod√®le entra√Æn√© et on le sauvegarde


```python
finetuned_directory = "./whisper-base-ch-perigueux"
model.save_pretrained(finetuned_directory)
processor.save_pretrained(finetuned_directory)
```

    Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
    Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}





    []



### Adaptation pour whisperLive
Pour notre reconnaissance en temps r√©el, nous utilisons les mod√®les faster-whisper, il nous faut donc les mettre dans ce format. Nous avons juste besoins de r√©cup√©rer le tokenizer.json du mod√®le que l'on a entrain√© et faire la transition par ctranslate2


```python
import os
import tokenizers

tokenizer_file = os.path.join(finetuned_directory, "tokenizer.json")
print(f"save  {tokenizer_file}", end="")
hf_tokenizer = tokenizers.Tokenizer.from_pretrained(model_name)
# Save the tokenizer to a file in the specified directory
hf_tokenizer.save(tokenizer_file)
print(" Ok")

```

    save  ./whisper-base-ch-perigueux/tokenizer.json Ok



```python
import shutil
import ctranslate2

# Chemin vers le dossier o√π le mod√®le est sauvegard√© dans le format
output_dir = f"{finetuned_directory}-faster-whisper"

# Valeurs possibles pour quantization:

#     "int8" : Quantification en 8 bits pour r√©duire la taille du mod√®le, souvent utilis√© pour les inf√©rences sur CPU.
#     "int16" : Quantification en 16 bits, un compromis entre la vitesse et la pr√©cision.
#     "float16" : Utilis√© pour les inf√©rences sur GPU, o√π les calculs peuvent √™tre effectu√©s en 16 bits flottants.
quantization = "int16"

# Convertir le mod√®le en CTranslate2
converter = ctranslate2.converters.TransformersConverter(finetuned_directory)
converter.convert(output_dir, quantization=quantization, force=True)

print(f"Le mod√®le a √©t√© converti et sauvegard√© dans {output_dir}")

shutil.copy(tokenizer_file, output_dir)
print(f"Fichier {tokenizer_file} copi√© dans {output_dir}")
```

    Le mod√®le a √©t√© converti et sauvegard√© dans ./whisper-base-ch-perigueux-faster-whisper
    Fichier ./whisper-base-ch-perigueux/tokenizer.json copi√© dans ./whisper-base-ch-perigueux-faster-whisper

